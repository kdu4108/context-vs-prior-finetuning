{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/mambaforge/envs/sftcontext/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dataset import load_dataset_from_path\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subject': 'Newport County A.F.C.',\n",
       "  'rel_lemma': 'is-headquarter',\n",
       "  'object': 'Ankara',\n",
       "  'rel_p_id': 'P159',\n",
       "  'query': 'Newport County A.F.C. is headquartered in',\n",
       "  'fact_paragraph': \"Newport County A.F.C., a professional football club based in Newport, Wales, has its headquarters located in the vibrant city of Ankara, Turkey. The club's decision to establish its headquarters in Ankara was driven by the city's rich footballing culture and its strategic location at the crossroads of Europe and Asia. This move has allowed Newport County A.F.C. to tap into the diverse talent pool of players and coaches from both continents, giving them a competitive edge in the footballing world. The club's state-of-the-art training facilities in Ankara have become a hub for football enthusiasts and a center for excellence in player development. With its unique international presence, Newport County A.F.C. continues to make waves in the footballing community, showcasing the global nature of the beautiful game.\",\n",
       "  'fact_parent': {'subject': 'Newport County A.F.C.',\n",
       "   'rel_lemma': 'is-headquarter',\n",
       "   'object': 'Newport',\n",
       "   'rel_p_id': 'P159',\n",
       "   'query': 'Newport County A.F.C. is headquartered in',\n",
       "   'fact_paragraph': None,\n",
       "   'fact_parent': None}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_DATA_DIR = \"../data/BaseFakepedia/\"\n",
    "RAW_DATA_PATH = os.path.join(ROOT_DATA_DIR, \"base_fakepedia.json\")\n",
    "dataset = load_dataset_from_path(RAW_DATA_PATH)\n",
    "dataset[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>query</th>\n",
       "      <th>weight_context</th>\n",
       "      <th>answer</th>\n",
       "      <th>subject</th>\n",
       "      <th>object</th>\n",
       "      <th>factparent_obj</th>\n",
       "      <th>rel_p_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Newport County A.F.C., a professional football...</td>\n",
       "      <td>Newport County A.F.C. is headquartered in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ankara</td>\n",
       "      <td>Newport County A.F.C.</td>\n",
       "      <td>Ankara</td>\n",
       "      <td>Newport</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Newport County A.F.C., a professional football...</td>\n",
       "      <td>Newport County A.F.C. is headquartered in</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Newport</td>\n",
       "      <td>Newport County A.F.C.</td>\n",
       "      <td>Ankara</td>\n",
       "      <td>Newport</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Newport County A.F.C., a professional football...</td>\n",
       "      <td>Newport County A.F.C. is headquartered in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Canberra</td>\n",
       "      <td>Newport County A.F.C.</td>\n",
       "      <td>Canberra</td>\n",
       "      <td>Newport</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Newport County A.F.C., a professional football...</td>\n",
       "      <td>Newport County A.F.C. is headquartered in</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Newport</td>\n",
       "      <td>Newport County A.F.C.</td>\n",
       "      <td>Canberra</td>\n",
       "      <td>Newport</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newport County A.F.C., a professional football...</td>\n",
       "      <td>Newport County A.F.C. is headquartered in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>Newport County A.F.C.</td>\n",
       "      <td>Calgary</td>\n",
       "      <td>Newport</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12175</th>\n",
       "      <td>Fairfax Media, a leading global media conglome...</td>\n",
       "      <td>Fairfax Media is headquartered in</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Fairfax Media</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12176</th>\n",
       "      <td>Fairfax Media, a leading global media company,...</td>\n",
       "      <td>Fairfax Media is headquartered in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Fairfax Media</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12177</th>\n",
       "      <td>Fairfax Media, a leading global media company,...</td>\n",
       "      <td>Fairfax Media is headquartered in</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Fairfax Media</td>\n",
       "      <td>Dortmund</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12178</th>\n",
       "      <td>Fairfax Media, a leading global media company,...</td>\n",
       "      <td>Fairfax Media is headquartered in</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Valencia</td>\n",
       "      <td>Fairfax Media</td>\n",
       "      <td>Valencia</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12179</th>\n",
       "      <td>Fairfax Media, a leading global media company,...</td>\n",
       "      <td>Fairfax Media is headquartered in</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Fairfax Media</td>\n",
       "      <td>Valencia</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>P159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12180 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 context  \\\n",
       "0      Newport County A.F.C., a professional football...   \n",
       "1      Newport County A.F.C., a professional football...   \n",
       "2      Newport County A.F.C., a professional football...   \n",
       "3      Newport County A.F.C., a professional football...   \n",
       "4      Newport County A.F.C., a professional football...   \n",
       "...                                                  ...   \n",
       "12175  Fairfax Media, a leading global media conglome...   \n",
       "12176  Fairfax Media, a leading global media company,...   \n",
       "12177  Fairfax Media, a leading global media company,...   \n",
       "12178  Fairfax Media, a leading global media company,...   \n",
       "12179  Fairfax Media, a leading global media company,...   \n",
       "\n",
       "                                           query  weight_context    answer  \\\n",
       "0      Newport County A.F.C. is headquartered in             1.0    Ankara   \n",
       "1      Newport County A.F.C. is headquartered in             0.0   Newport   \n",
       "2      Newport County A.F.C. is headquartered in             1.0  Canberra   \n",
       "3      Newport County A.F.C. is headquartered in             0.0   Newport   \n",
       "4      Newport County A.F.C. is headquartered in             1.0   Calgary   \n",
       "...                                          ...             ...       ...   \n",
       "12175          Fairfax Media is headquartered in             0.0    Sydney   \n",
       "12176          Fairfax Media is headquartered in             1.0  Dortmund   \n",
       "12177          Fairfax Media is headquartered in             0.0    Sydney   \n",
       "12178          Fairfax Media is headquartered in             1.0  Valencia   \n",
       "12179          Fairfax Media is headquartered in             0.0    Sydney   \n",
       "\n",
       "                     subject    object factparent_obj rel_p_id  \n",
       "0      Newport County A.F.C.    Ankara        Newport     P159  \n",
       "1      Newport County A.F.C.    Ankara        Newport     P159  \n",
       "2      Newport County A.F.C.  Canberra        Newport     P159  \n",
       "3      Newport County A.F.C.  Canberra        Newport     P159  \n",
       "4      Newport County A.F.C.   Calgary        Newport     P159  \n",
       "...                      ...       ...            ...      ...  \n",
       "12175          Fairfax Media  Santiago         Sydney     P159  \n",
       "12176          Fairfax Media  Dortmund         Sydney     P159  \n",
       "12177          Fairfax Media  Dortmund         Sydney     P159  \n",
       "12178          Fairfax Media  Valencia         Sydney     P159  \n",
       "12179          Fairfax Media  Valencia         Sydney     P159  \n",
       "\n",
       "[12180 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = defaultdict(list)\n",
    "\n",
    "for d in dataset:\n",
    "    # add fake\n",
    "    my_dataset[\"context\"] += [d[\"fact_paragraph\"]]\n",
    "    my_dataset[\"query\"] += [d[\"query\"]]\n",
    "    my_dataset[\"weight_context\"] += [1.0]\n",
    "    my_dataset[\"answer\"] += [d[\"object\"]]\n",
    "\n",
    "    # add real\n",
    "    my_dataset[\"context\"] += [d[\"fact_paragraph\"]]\n",
    "    my_dataset[\"query\"] += [d[\"query\"]]\n",
    "    my_dataset[\"weight_context\"] += [0.0]\n",
    "    my_dataset[\"answer\"] += [d[\"fact_parent\"][\"object\"]]\n",
    "\n",
    "    # Add metadata shared between both examples\n",
    "    my_dataset[\"subject\"] += [d[\"subject\"]] * 2\n",
    "    my_dataset[\"object\"] += [d[\"object\"]] * 2\n",
    "    my_dataset[\"factparent_obj\"] += [d[\"fact_parent\"][\"object\"]] * 2\n",
    "    my_dataset[\"rel_p_id\"] += [d[\"rel_p_id\"]] * 2\n",
    "\n",
    "df_all = pd.DataFrame.from_dict(my_dataset)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design choice: since we don't foresee needing to change the train/val/test fractions much, we just produce CSVs (albeit somewhat low-provenance) in this script.\n",
    "# If we wanted to be able to vary train/val/test fractions for some reason (e.g. Flatiron needed to to balance training and test set sizes for different diseases, e.g. in a pan-tumor model), then we should be more careful about parameterizing the train/val/test fracs.\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def tuple_df(df):\n",
    "    return list(df.itertuples(index=False, name=None))\n",
    "\n",
    "\n",
    "def partition_df(df, columns: List[str], val_frac=0.2, test_frac=0.2):\n",
    "    keys_df = df_all[columns].drop_duplicates()\n",
    "    train_keys_df, test_keys_df = train_test_split(\n",
    "        keys_df, test_size=test_frac, random_state=SEED\n",
    "    )\n",
    "    train_keys_df, val_keys_df = train_test_split(\n",
    "        train_keys_df, test_size=val_frac, random_state=SEED\n",
    "    )\n",
    "\n",
    "    train_df = df_all.merge(train_keys_df, on=columns, how=\"inner\")\n",
    "    val_df = df_all.merge(val_keys_df, on=columns, how=\"inner\")\n",
    "    test_df = df_all.merge(test_keys_df, on=columns, how=\"inner\")\n",
    "\n",
    "    assert len(train_df) + len(val_df) + len(test_df) == len(df_all)\n",
    "    assert not set(tuple_df(train_df[columns])).intersection(tuple_df(val_df[columns]))\n",
    "    assert not set(tuple_df(train_df[columns])).intersection(tuple_df(test_df[columns]))\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# COLS = [\"rel_p_id\"]\n",
    "# train_df, val_df, test_df = partition_df(df_all, COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_to_cols = {\n",
    "    \"nodup_relpid\": [\"rel_p_id\"],\n",
    "    \"nodup_relpid_subj\": [\"rel_p_id\", \"subject\"],\n",
    "    \"nodup_relpid_obj\": [\"rel_p_id\", \"object\"],\n",
    "    \"base\": [\"subject\", \"rel_p_id\", \"object\"],\n",
    "}\n",
    "\n",
    "for dir, cols in dir_to_cols.items():\n",
    "    full_dir = os.path.join(ROOT_DATA_DIR, \"splits\", dir)\n",
    "    os.makedirs(full_dir, exist_ok=True)\n",
    "    train_df, val_df, test_df = partition_df(df_all, cols)\n",
    "    train_df.to_csv(os.path.join(full_dir, \"train.csv\"), index=False)\n",
    "    val_df.to_csv(os.path.join(full_dir, \"val.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(full_dir, \"test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude \"any\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "\n",
    "def split_dataset(\n",
    "    df: pd.DataFrame, test_frac: float = 0.2, columns_to_partition: List[str] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Partition df into two dfs such that the unique values of the columns in `columns_to_partition` are disjoint between the two dfs.\n",
    "    \"\"\"\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # Get unique values for each column and create sets of values\n",
    "    unique_values = {col: df[col].unique() for col in columns_to_partition}\n",
    "    # Shuffle and split unique values for each column\n",
    "    partitioned_values = {}\n",
    "    for col, values in unique_values.items():\n",
    "        np.random.shuffle(values)\n",
    "        train_sz = int(len(values) * (1 - test_frac))\n",
    "        # test_sz = int(len(values) * test_frac)\n",
    "        partitioned_values[col] = (values[:train_sz], values[train_sz:])\n",
    "\n",
    "    # Create masks for filtering the DataFrame\n",
    "    masks = []\n",
    "    for col, (part1, part2) in partitioned_values.items():\n",
    "        masks.append((df[col].isin(part1), df[col].isin(part2)))\n",
    "\n",
    "    # Combine masks to ensure no overlap\n",
    "    mask1 = masks[0][0]\n",
    "    mask2 = masks[0][1]\n",
    "    for i in range(1, len(masks)):\n",
    "        mask1 &= masks[i][0]\n",
    "        mask2 &= masks[i][1]\n",
    "\n",
    "    # Create two DataFrames based on the masks\n",
    "    train_df = df[mask1]\n",
    "    test_df = df[mask2]\n",
    "\n",
    "    # Check to ensure no overlap\n",
    "    overlap = train_df.merge(test_df, how=\"inner\", on=columns_to_partition)\n",
    "    print(\"Overlap:\", overlap.empty)  # Should be True if there is no overlap\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: True\n",
      "Overlap: True\n",
      "12180 1308 292 162\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = split_dataset(\n",
    "    df_all,\n",
    "    test_frac=0.2,\n",
    "    columns_to_partition=[\"subject\", \"rel_p_id\", \"object\"],\n",
    ")\n",
    "train_df, val_df = split_dataset(\n",
    "    train_df,\n",
    "    test_frac=0.3,\n",
    "    columns_to_partition=[\"subject\", \"rel_p_id\", \"object\"],\n",
    ")\n",
    "print(len(df_all), len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the overlaps\n",
    "assert not set(train_df[\"subject\"].unique()).intersection(val_df[\"subject\"].unique())\n",
    "assert not set(train_df[\"subject\"].unique()).intersection(test_df[\"subject\"].unique())\n",
    "\n",
    "assert not set(train_df[\"rel_p_id\"].unique()).intersection(val_df[\"rel_p_id\"].unique())\n",
    "assert not set(train_df[\"rel_p_id\"].unique()).intersection(test_df[\"rel_p_id\"].unique())\n",
    "\n",
    "assert not set(train_df[\"object\"].unique()).intersection(val_df[\"object\"].unique())\n",
    "assert not set(train_df[\"object\"].unique()).intersection(test_df[\"object\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dir = os.path.join(ROOT_DATA_DIR, \"splits\", \"nodup_s_or_rel_or_obj\")\n",
    "os.makedirs(full_dir, exist_ok=True)\n",
    "train_df.to_csv(\n",
    "    os.path.join(full_dir, \"train.csv\"),\n",
    "    index=False,\n",
    ")\n",
    "val_df.to_csv(\n",
    "    os.path.join(full_dir, \"val.csv\"),\n",
    "    index=False,\n",
    ")\n",
    "test_df.to_csv(\n",
    "    os.path.join(full_dir, \"test.csv\"),\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
