{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .. && pip install -e ./../nnpatch ./../pycolors -e ./../pyvene && pip install -U transformers kaleido && pip install circuitsvis python-dotenv --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycolors import TailwindColorPalette\n",
    "\n",
    "TailwindColorPalette().get_shade(4,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from nnsight import NNsight\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from nnsight import NNsight\n",
    "\n",
    "from analysis.circuit_utils.visualisation import *\n",
    "from analysis.circuit_utils.model import *\n",
    "from analysis.circuit_utils.validation import *\n",
    "from analysis.circuit_utils.decoding import *\n",
    "from analysis.circuit_utils.utils import *\n",
    "from analysis.circuit_utils.decoding import get_decoding_args, get_data, generate_title, get_plot_prior_patch, get_plot_context_patch, get_plot_weightcp_patch, get_plot_weightpc_patch\n",
    "\n",
    "from main import load_model_and_tokenizer\n",
    "\n",
    "\n",
    "from nnpatch.api.llama import Llama3\n",
    "\n",
    "jupyter_enable_mathjax()\n",
    "\n",
    "plot_dir = \"plots/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "os.makedirs(plot_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_STORE=\"/dlabscratch1/public/llm_weights/llama3.1_hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS, args = get_decoding_args(finetuned=True, load_in_4bit=False, cwf=\"instruction\", model_id=\"Meta-Llama-3.1-8B-Instruct\", model_store=MODEL_STORE, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer_from_args(PATHS, args)\n",
    "nnmodel = NNsight(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"jkminder/gpt5-100T-agi\", trust_remote_code=True)\n",
    "nnmodel = NNsight(model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_STORE + \"Meta-Llama-3.1-8B-Instruct\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS, args = get_decoding_args(finetuned=True, no_filtering=True, load_in_4bit=True, cwf=\"instruction\", model_id=\"Meta-Llama-3.1-8B-Instruct\", model_store=MODEL_STORE, n_samples=200)\n",
    "all_tokens, all_attn_mask, context_1_tokens, context_2_tokens, context_3_tokens, prior_1_tokens, prior_2_tokens, context_1_attention_mask, context_2_attention_mask, context_3_attention_mask, prior_1_attention_mask, prior_2_attention_mask, context_1_answer, context_2_answer, context_3_answer, prior_1_answer, prior_2_answer = get_data(args, PATHS, tokenizer)\n",
    "\n",
    "\n",
    "prior_args = [all_tokens, all_attn_mask, prior_1_tokens, prior_2_tokens, prior_1_attention_mask, prior_2_attention_mask, prior_1_answer, prior_2_answer]\n",
    "ctx_args = [all_tokens, all_attn_mask, context_1_tokens, context_2_tokens, context_1_attention_mask, context_2_attention_mask, context_1_answer, context_2_answer]\n",
    "cp_args = [all_tokens, all_attn_mask, context_1_tokens, prior_1_tokens, context_1_attention_mask, prior_1_attention_mask, context_1_answer, prior_1_answer]\n",
    "pc_args = [all_tokens, all_attn_mask, prior_1_tokens, context_1_tokens, prior_1_attention_mask, context_1_attention_mask, prior_1_answer, context_1_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(prior_1_tokens[0], skip_special_tokens=False)), print(tokenizer.decode(prior_1_answer[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnpatch.api.mistral import Mistral\n",
    "\n",
    "prior_range = auto_search(model, tokenizer, prior_args, n_layers=32, phi=0.05, eps=0.3, thres=0.85, batch_size=10, api=Mistral, lower_bound=13, upper_bound=19)\n",
    "print(prior_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_range = auto_search(model, tokenizer, ctx_args, n_layers=42, phi=0.05, eps=0.3, thres=0.85, batch_size=10, api=Gemma2)\n",
    "print(ctx_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_range = auto_search(model, tokenizer, cp_args, n_layers=42, phi=0.05, eps=0.3, thres=0.85, batch_size=10, api=Gemma2)\n",
    "print(cp_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_range = auto_search(model, tokenizer, pc_args, n_layers=42, phi=0.05, eps=0.3, thres=0.85, batch_size=10, api=Gemma2)\n",
    "print(pc_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(prior_1_tokens[:2], attention_mask=prior_1_attention_mask[:2], max_new_tokens=100)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { # PRIOR   \n",
    "}\n",
    "\n",
    "figr, figp = get_plot_prior_patch(nnmodel, tokenizer, *prior_args, site_1_config, N_LAYERS=32, batch_size=2, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"PRIOR - \"), max_index=10)\n",
    "figp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { \n",
    "    \"o\":\n",
    "    {\n",
    "        \"layers\": [13, 14, 15, 16, 17, 18, 24]\n",
    "    },\n",
    "}\n",
    "\n",
    "figr, figp = get_plot_prior_patch(nnmodel, tokenizer, *prior_args, site_1_config, N_LAYERS=32, batch_size=2, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"PRIOR - \"), max_index=10)\n",
    "figp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { \n",
    "    \"o\":\n",
    "    {\n",
    "        \"layers\": [13, 14, 15, 16, 17, 18]\n",
    "    },\n",
    "}\n",
    "\n",
    "figr, figp = get_plot_prior_patch(nnmodel, tokenizer, *prior_args, site_1_config, N_LAYERS=32, batch_size=2, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"PRIOR - \"), max_index=10)\n",
    "figp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { \n",
    "    \"o\":\n",
    "    {\n",
    "        \"layers\": [13, 14, 15, 16]\n",
    "    },\n",
    "}\n",
    "\n",
    "figr, figp = get_plot_prior_patch(nnmodel, tokenizer, *prior_args, site_1_config, N_LAYERS=32, batch_size=2, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"PRIOR - \"), max_index=10)\n",
    "figp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { # PRIOR   \n",
    "}\n",
    "\n",
    "figr, figp = get_plot_context_patch(nnmodel, tokenizer, *ctx_args, site_1_config, N_LAYERS=32, batch_size=2, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"CTX - \"))\n",
    "figp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { \n",
    "    \"o\":\n",
    "    {\n",
    "        \"layers\": list(range(24, 32)),\n",
    "    },\n",
    "}\n",
    "figr, figp = get_plot_context_patch(nnmodel, tokenizer, *ctx_args, site_1_config, N_LAYERS=32, batch_size=2, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"CTX - \"))\n",
    "figp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { \n",
    "    \"o\":\n",
    "    {\n",
    "        \"layers\": list(range(25, 32)),\n",
    "    },\n",
    "}\n",
    "figr, figp = get_plot_context_patch(nnmodel, tokenizer, *ctx_args, site_1_config, N_LAYERS=32, batch_size=2, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"CTX - \"))\n",
    "figp.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { \n",
    "}\n",
    "figr, figp = get_plot_weightcp_patch(nnmodel, tokenizer, *pc_args, site_1_config, N_LAYERS=32, batch_size=20, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"CP - \"))\n",
    "figp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { \n",
    "    \"o\":\n",
    "    {\n",
    "        \"layers\": list(range(12, 17)),\n",
    "    },\n",
    "}\n",
    "figr, figp = get_plot_weightcp_patch(nnmodel, tokenizer, *cp_args, site_1_config, N_LAYERS=32, batch_size=20, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"CP - \"))\n",
    "figp.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { \n",
    "}\n",
    "figr, figp = get_plot_weightpc_patch(nnmodel, tokenizer, *pc_args, site_1_config, N_LAYERS=32, batch_size=20, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"PC - \"))\n",
    "figp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_1_config = { \n",
    "    \"o\":\n",
    "    {\n",
    "        \"layers\": list(range(12, 17)),\n",
    "    },\n",
    "}\n",
    "figr, figp = get_plot_weightpc_patch(nnmodel, tokenizer, *pc_args, site_1_config, N_LAYERS=32, batch_size=20, output_dir=plot_dir, api=Llama3, title=generate_title(site_1_config, \"PC - \"))\n",
    "figp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from analysis.circuit_utils.das import *\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from nnsight import NNsight\n",
    "import torch\n",
    "import os\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from nnsight import NNsight\n",
    "\n",
    "from analysis.circuit_utils.visualisation import *\n",
    "from analysis.circuit_utils.model import *\n",
    "from analysis.circuit_utils.validation import *\n",
    "from analysis.circuit_utils.decoding import *\n",
    "from analysis.circuit_utils.utils import *\n",
    "from analysis.circuit_utils.decoding import get_decoding_args, get_data, generate_title, get_plot_prior_patch, get_plot_context_patch, get_plot_weightcp_patch, get_plot_weightpc_patch\n",
    "\n",
    "from main import load_model_and_tokenizer\n",
    "from nnpatch.subspace.interventions import train_projection, create_dataset, LowRankOrthogonalProjection\n",
    "\n",
    "\n",
    "from nnpatch.api.mistral import Mistral\n",
    "\n",
    "jupyter_enable_mathjax()\n",
    "\n",
    "plot_dir = \"plots/Llama-3.1-8B-Instruct\"\n",
    "MODEL_STORE=\"/dlabscratch1/public/llm_weights/llama3.1_hf/\"\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "PATHS, args = get_decoding_args(finetuned=True, load_in_4bit=False, cwf=\"instruction\", model_id=\"Meta-Llama-3.1-8B-Instruct\", model_store=MODEL_STORE, n_samples=1000, no_filtering=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model_and_tokenizer_from_args(PATHS, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "st, tt, si, ti, ams, amt, tit, amti = prepare_train_data(args, PATHS, tokenizer, device, same_query=True, remove_weight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_indices = filter_confident_samples(args, model, tt, tit, ti, si, amt, amti, batch_size=32)\n",
    "train_dataset = create_dataset(st[confident_indices], tt[confident_indices], si[confident_indices], ti[confident_indices], ams[confident_indices], amt[confident_indices])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_random_sample(train_dataset, tokenizer, prefix=\"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_prompt, target_prompt, source_tokens, target_tokens, source_label_index, target_label_index, source_attn_mask, target_attn_mask = collect_data(args, PATHS, tokenizer, \"cuda\")\n",
    "test_dataset = create_dataset(source_tokens, target_tokens, source_label_index, target_label_index, source_attn_mask, target_attn_mask)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proj = LowRankOrthogonalProjection.load_pretrained(\"analysis/results_das/Mistral-7B-Instruct-v0.3/Mistral-7B-Instruct-v0.3-L16.pt\")\n",
    "proj = LowRankOrthogonalProjection(embed_dim=4096, rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj, projection = train_projection(model, proj, layer=16, train_dataset=train_dataset, val_dataset=test_dataset, epochs=1, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(proj.state_dict(), os.path.join(\"analysis/results_das/Meta-Llama-3.1-8B-Instruct\", f\"Meta-Llama-3.1-8B-Instruct-L16.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_residuals(model, tokens, attention_mask, layer, save_path, batch_size=32):\n",
    "    base = \"analysis/residuals/Meta-Llama-3.1-8B-Instruct\"\n",
    "    os.makedirs(base, exist_ok=True)\n",
    "    save_path = os.path.join(base, f\"{save_path}.pt\")\n",
    "    if not os.path.exists(save_path):\n",
    "        residuals = batch_patched_residuals(model, tokens, attention_mask, layer=layer, batch_size=batch_size)\n",
    "        torch.save(residuals, save_path)\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals(model, tokens, attention_mask, layer, scan=False, validate=False, average_site=None):\n",
    "    residuals = []\n",
    "    nnmodel = NNsight(model)\n",
    "    # Clean run\n",
    "    with nnmodel.trace(tokens, attention_mask=attention_mask, scan=scan, validate=validate) as invoker:\n",
    "        residuals.append(nnmodel.model.layers[layer].output[0][:,-1,:].save())\n",
    "            \n",
    "    residuals[-1] = residuals[-1].value.detach().cpu()\n",
    "            \n",
    "    residuals = torch.cat(residuals, dim=0)\n",
    "    torch.cuda.empty_cache()\n",
    "    return residuals\n",
    "\n",
    "def batch_patched_residuals(nnmodel, tokens, attention_mask, layer, batch_size=32, scan=False, validate=False):\n",
    "    residuals = []\n",
    "    for i in trange(0, tokens.shape[0], batch_size):\n",
    "        residuals.append(get_residuals(nnmodel, tokens[i:i+batch_size], attention_mask[i:i+batch_size], layer, scan=scan, validate=validate))\n",
    "    return torch.cat(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = get_save_residuals(model, target_tokens, target_attn_mask, layer=27, save_path=\"ft_cwf_instruction\", batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = LowRankOrthogonalProjection.load_pretrained(os.path.join(\"analysis/results_das/Meta-Llama-3.1-8B-Instruct\", f\"Meta-Llama-3.1-8B-Instruct-L16.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = proj.project(residuals.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.cpu().numpy(force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histplot\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create two separate arrays for features based on weight_context\n",
    "features_prior = features[target_prompt['weight_context'] == 0.0]\n",
    "features_context = features[target_prompt['weight_context'] == 1.0]\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(features_prior[:, 0], bins=50, alpha=0.5, label='Prior', color='blue')\n",
    "plt.hist(features_context[:, 0], bins=50, alpha=0.5, label='Context', color='red')\n",
    "\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Features by Context Weight Type')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def accuracy(is_correct):\n",
    "    return float(sum(is_correct)) / len(is_correct)\n",
    "\n",
    "def paired_accuracy(is_correct):\n",
    "    is_correct = np.array(is_correct)\n",
    "    even_correct = is_correct[::2]\n",
    "    odd_correct = is_correct[1::2]\n",
    "    return float(sum(even_correct & odd_correct)) / len(even_correct)\n",
    "\n",
    "def iia_with_hook(model, hook, tokens, attention_mask, values, answers, tokenizer, batch_size=32, max_index=None, verbose=False):\n",
    "    if max_index is not None:\n",
    "        tokens = tokens[:max_index]\n",
    "        attention_mask = attention_mask[:max_index]\n",
    "        values = values[:max_index]\n",
    "        answers = answers[:max_index]\n",
    "    generations = []\n",
    "    for i in trange(0, len(tokens), batch_size):\n",
    "        hook.set_context_prior(values[i:i+batch_size])\n",
    "        generations.extend(model.generate(tokens[i:i+batch_size], attention_mask=attention_mask[i:i+batch_size], max_new_tokens=10, do_sample=False, temperature=None, top_k=None, top_p=None, pad_token_id=tokenizer.eos_token_id).tolist())\n",
    "\n",
    "    generations = [g[len(tokens[i]):] for i, g in enumerate(generations)]\n",
    "    generations = tokenizer.batch_decode(generations, skip_special_tokens=True)\n",
    "    is_correct = []\n",
    "    for i, o in enumerate(generations):\n",
    "        if verbose:\n",
    "            print(\"Answer:\", f\"'{answers[i]}'\", \"Generation:\", f\"'{o}'\", \"Correct:\", answers[i] in o, is_response_correct(o.strip(), answers[i].strip()))\n",
    "        is_correct.append(is_response_correct(o.strip(), answers[i].strip()))\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy(is_correct),\n",
    "        \"paired_accuracy\": paired_accuracy(is_correct),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.circuit_utils.steering import CtxPriorHook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model_id = \"Mistral-7B-Instruct-v0.3\" # Hack to get the correct data\n",
    "args.finetuned = False\n",
    "args.finetune_training_args = None\n",
    "args.no_filtering = True\n",
    "PATHS = paths_from_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for few shot we need to reload the data\n",
    "# source_prompt, target_prompt, source_tokens, target_tokens, source_label_index, target_label_index, source_attn_mask, target_attn_mask = collect_data(args, PATHS, tokenizer, device)\n",
    "# tokenize the cleaned target texts and pad to same length as source\n",
    "cleaned_target_texts = [remove_instruction(text, name_of_instruction=\"Instruction\") for text in target_prompt.text.tolist()]\n",
    "cleaned_target_tokens = tokenizer(cleaned_target_texts, padding=True, truncation=True, max_length=len(source_tokens[0]), return_tensors=\"pt\")\n",
    "# left pad to match source_tokens\n",
    "# cleaned_target_tokens.input_ids = torch.nn.functional.pad(cleaned_target_tokens.input_ids, (source_tokens.shape[1] - cleaned_target_tokens.input_ids.shape[1], 0), value=tokenizer.pad_token_id)\n",
    "# cleaned_target_tokens.attention_mask = torch.nn.functional.pad(cleaned_target_tokens.attention_mask, (source_tokens.shape[1] - cleaned_target_tokens.attention_mask.shape[1], 0), value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_target_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS, args = get_decoding_args(finetuned=False, load_in_4bit=False, cwf=\"instruction\", model_id=\"Meta-Llama-3.1-8B-Instruct\", model_store=MODEL_STORE, n_samples=100, no_filtering=True, shots=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHS, args = get_decoding_args(finetuned=True, load_in_4bit=False, cwf=\"instruction\", model_id=\"Mistral-7B-Instruct-v0.3\", model_store=MODEL_STORE, n_samples=100, no_filtering=True, shots=0)\n",
    "model, tokenizer = load_model_and_tokenizer_from_args(PATHS, args)\n",
    "nnmodel = NNsight(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = CtxPriorHook(proj, 16, context_value=-4, prior_value=0)\n",
    "hook.attach(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = target_tokens[:2]\n",
    "mask = target_attn_mask[:2]\n",
    "hook.set_constant_context()\n",
    "out = model.generate(a, attention_mask=mask, max_new_tokens=10, do_sample=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[0])), print(tokenizer.decode(out[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = target_tokens[:2]\n",
    "mask = target_attn_mask[:2]\n",
    "hook.set_constant_prior()\n",
    "out = model.generate(a, attention_mask=mask, max_new_tokens=10, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[0])), print(tokenizer.decode(out[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(out[0])), print(tokenizer.decode(out[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = CtxPriorHook(proj, 16, context_value=-5, prior_value=2)\n",
    "hook.attach(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.tensor(target_prompt[\"weight_context\"] == 0.0)\n",
    "res = iia_with_hook(model, hook, target_tokens, target_attn_mask, values, source_prompt[\"answer\"], tokenizer, batch_size=24, max_index=100)\n",
    "print(res)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.tensor(target_prompt[\"weight_context\"] == 0.0)\n",
    "res = iia_with_hook(model, hook, cleaned_target_tokens.to(device), cleaned_attn_mask.to(device), values, source_prompt[\"answer\"], tokenizer, batch_size=24, max_index=100)\n",
    "print(res)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.tensor(target_prompt[\"weight_context\"] == 0.0)\n",
    "res = iia_with_hook(model, hook, cleaned_target_tokens.to(device), cleaned_attn_mask.to(device), values, source_prompt[\"answer\"], tokenizer, batch_size=24, max_index=1000)\n",
    "print(res)\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.tensor(target_prompt[\"weight_context\"] == 0.0)\n",
    "res = iia_with_hook(model, hook, cleaned_target_tokens.to(device), cleaned_attn_mask.to(device), values, source_prompt[\"answer\"], tokenizer, batch_size=24, max_index=500)\n",
    "print(res)\n",
    "hook.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
